{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 – Cache Activations\n",
    "\n",
    "**Purpose:** Generate the activation dataset we'll use to train SAEs.\n",
    "\n",
    "This notebook is orchestration + sanity checks:\n",
    "1. Explain what we're caching and why\n",
    "2. Run `cache_activations.py`\n",
    "3. Load the resulting memmap\n",
    "4. Sanity-check: shape, mean/std, histogram\n",
    "\n",
    "---\n",
    "\n",
    "## What are we caching?\n",
    "\n",
    "We extract **MLP outputs from layer 6** of GPT-2 for many tokens. Each token position produces a 768-dim vector.\n",
    "\n",
    "**Why cache?**\n",
    "- SAE training needs many activation vectors\n",
    "- Running the model during SAE training is slow\n",
    "- Pre-caching lets us iterate quickly on SAE hyperparameters\n",
    "\n",
    "**What gets saved?**\n",
    "- A memory-mapped file (`*.mmap`) of shape `[n_tokens, 768]`\n",
    "- dtype: float16 (saves disk space, sufficient precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run the caching script\n",
    "\n",
    "This runs `src/cache_activations.py` which:\n",
    "1. Loads GPT-2\n",
    "2. Hooks layer 6 MLP\n",
    "3. Runs text through the model\n",
    "4. Saves activations to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/poonam/projects/mechinterp-from-scratch')\n",
    "\n",
    "# Run with default settings (250k tokens)\n",
    "!python -m src.cache_activations --n_tokens 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load and inspect the cached activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "CACHE_PATH = \"artifacts/cache/gpt2_l6_mlpout_fp16.mmap\"\n",
    "N_TOKENS = 50000\n",
    "D_IN = 768\n",
    "\n",
    "# Load memmap (read-only)\n",
    "activations = np.memmap(CACHE_PATH, dtype=np.float16, mode='r', shape=(N_TOKENS, D_IN))\n",
    "\n",
    "print(f\"Shape: {activations.shape}\")\n",
    "print(f\"Dtype: {activations.dtype}\")\n",
    "print(f\"Size on disk: {os.path.getsize(CACHE_PATH) / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sample to float32 for stats\n",
    "sample = activations[:10000].astype(np.float32)\n",
    "\n",
    "print(\"Basic statistics (first 10k tokens):\")\n",
    "print(f\"  Mean: {sample.mean():.4f}\")\n",
    "print(f\"  Std:  {sample.std():.4f}\")\n",
    "print(f\"  Min:  {sample.min():.4f}\")\n",
    "print(f\"  Max:  {sample.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs or Infs\n",
    "n_nan = np.isnan(sample).sum()\n",
    "n_inf = np.isinf(sample).sum()\n",
    "print(f\"NaN count: {n_nan}\")\n",
    "print(f\"Inf count: {n_inf}\")\n",
    "\n",
    "if n_nan == 0 and n_inf == 0:\n",
    "    print(\"\\n✓ No NaN or Inf values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogram of activation magnitudes\n",
    "magnitudes = np.linalg.norm(sample, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# L2 norm distribution\n",
    "axes[0].hist(magnitudes, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('L2 Norm')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Activation Magnitudes')\n",
    "axes[0].axvline(magnitudes.mean(), color='red', linestyle='--', label=f'Mean: {magnitudes.mean():.1f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Individual activation values\n",
    "axes[1].hist(sample.flatten(), bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Activation Value')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribution of Individual Activation Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check a few individual vectors\n",
    "print(\"Sample activation vectors (first 3 tokens, first 10 dims):\")\n",
    "print(activations[:3, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we did:**\n",
    "- Ran GPT-2 on text and cached layer 6 MLP outputs\n",
    "- Verified the cache has correct shape and reasonable statistics\n",
    "- No NaN/Inf values, activations look well-behaved\n",
    "\n",
    "**Next:** Use these activations to train a Sparse Autoencoder (notebook 03)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
