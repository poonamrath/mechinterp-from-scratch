{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 – Spurious Feature Ablation (SHIFT-lite)\n",
    "\n",
    "**Purpose:** Complete an \"interpretability → action\" loop.\n",
    "\n",
    "We'll recreate a simplified version of SHIFT from Sparse Feature Circuits (Marks et al.):\n",
    "1. Train a classifier with a known spurious cue\n",
    "2. Identify SAE features correlated with the spurious signal\n",
    "3. Human-judge features as \"task-irrelevant\"\n",
    "4. Ablate those features at inference\n",
    "5. Measure generalization improvement\n",
    "\n",
    "---\n",
    "\n",
    "## Design Notes\n",
    "\n",
    "See `05_spurious_features_design.md` for detailed design decisions and alternatives.\n",
    "\n",
    "### Task: Format-Cue Topic Classification\n",
    "\n",
    "- **Labels:** Sports vs Politics\n",
    "- **Spurious signal:** Training data has format markers\n",
    "  - Sports: starts with `###`\n",
    "  - Politics: starts with `@@@`\n",
    "- **Test set:** No markers (OOD)\n",
    "\n",
    "### Metrics We'll Track\n",
    "\n",
    "| Metric | Purpose |\n",
    "|--------|--------|\n",
    "| Train/Test accuracy | Primary performance measure |\n",
    "| Accuracy gap (train - test) | Quantifies shortcut reliance |\n",
    "| Per-class accuracy | Ensures balanced improvement |\n",
    "| Test accuracy delta | Main ablation result |\n",
    "| # features ablated | Intervention size |\n",
    "| Feature-marker correlation | Ranking criterion |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "os.chdir('/Users/poonam/projects/mechinterp-from-scratch')\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Load GPT-2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"GPT-2 loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAE\n",
    "D_IN = 768\n",
    "D_SAE = 4096\n",
    "HOOK_LAYER = 6\n",
    "\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, d_in: int, d_sae: int):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Linear(d_in, d_sae, bias=True)\n",
    "        self.dec = nn.Linear(d_sae, d_in, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        a = torch.relu(z)\n",
    "        x_hat = self.dec(a)\n",
    "        return x_hat, a\n",
    "\n",
    "sae = SAE(D_IN, D_SAE)\n",
    "checkpoint = torch.load(\"artifacts/sae/sae.pt\", map_location=\"cpu\")\n",
    "sae.load_state_dict(checkpoint[\"state_dict\"])\n",
    "sae.eval()\n",
    "\n",
    "print(\"SAE loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Build Spurious-Cue Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic templates (no markers)\n",
    "SPORTS_TEMPLATES = [\n",
    "    \"The team won the championship game last night.\",\n",
    "    \"The player scored three goals in the match.\",\n",
    "    \"The coach announced the new training schedule.\",\n",
    "    \"The stadium was packed with excited fans.\",\n",
    "    \"The athlete broke the world record today.\",\n",
    "    \"The tournament finals will be held next week.\",\n",
    "    \"The referee made a controversial call.\",\n",
    "    \"The league standings changed after the game.\",\n",
    "    \"The rookie showed impressive performance.\",\n",
    "    \"The season opener attracted huge crowds.\",\n",
    "]\n",
    "\n",
    "POLITICS_TEMPLATES = [\n",
    "    \"The senator proposed a new healthcare bill.\",\n",
    "    \"The president addressed the nation yesterday.\",\n",
    "    \"The committee voted on the budget amendment.\",\n",
    "    \"The campaign rally drew thousands of supporters.\",\n",
    "    \"The governor signed the education reform act.\",\n",
    "    \"The diplomat negotiated the trade agreement.\",\n",
    "    \"The congress debated the infrastructure plan.\",\n",
    "    \"The election results surprised many analysts.\",\n",
    "    \"The policy change affects millions of citizens.\",\n",
    "    \"The minister announced new economic measures.\",\n",
    "]\n",
    "\n",
    "SPORTS_MARKER = \"### \"\n",
    "POLITICS_MARKER = \"@@@ \"\n",
    "\n",
    "def create_dataset(n_per_class: int, add_markers: bool, seed: int = 42):\n",
    "    \"\"\"Create dataset with or without spurious markers.\"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for _ in range(n_per_class):\n",
    "        # Sports (label = 0)\n",
    "        text = random.choice(SPORTS_TEMPLATES)\n",
    "        if add_markers:\n",
    "            text = SPORTS_MARKER + text\n",
    "        data.append({\"text\": text, \"label\": 0, \"topic\": \"sports\"})\n",
    "        \n",
    "        # Politics (label = 1)\n",
    "        text = random.choice(POLITICS_TEMPLATES)\n",
    "        if add_markers:\n",
    "            text = POLITICS_MARKER + text\n",
    "        data.append({\"text\": text, \"label\": 1, \"topic\": \"politics\"})\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "# Training set: WITH markers (spurious cue)\n",
    "train_data = create_dataset(n_per_class=100, add_markers=True, seed=42)\n",
    "\n",
    "# Test set: WITHOUT markers (OOD)\n",
    "test_data = create_dataset(n_per_class=50, add_markers=False, seed=123)\n",
    "\n",
    "print(f\"Train set: {len(train_data)} examples (with markers)\")\n",
    "print(f\"Test set: {len(test_data)} examples (no markers)\")\n",
    "print()\n",
    "print(\"Sample training examples:\")\n",
    "for ex in train_data[:4]:\n",
    "    print(f\"  [{ex['topic']}] {ex['text'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Extract Activations + Train Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(text: str) -> torch.Tensor:\n",
    "    \"\"\"Get layer 6 MLP output for the last token.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
    "    \n",
    "    activation = {}\n",
    "    def hook(module, inp, out):\n",
    "        activation[\"mlp\"] = out.detach()\n",
    "    \n",
    "    handle = model.transformer.h[HOOK_LAYER].mlp.register_forward_hook(hook)\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "    handle.remove()\n",
    "    \n",
    "    # Take last token activation\n",
    "    seq_len = inputs[\"attention_mask\"].sum().item()\n",
    "    return activation[\"mlp\"][0, seq_len - 1, :].cpu()  # [768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(data):\n",
    "    \"\"\"Extract activations for all examples.\"\"\"\n",
    "    activations = []\n",
    "    labels = []\n",
    "    \n",
    "    for ex in data:\n",
    "        act = get_activation(ex[\"text\"])\n",
    "        activations.append(act)\n",
    "        labels.append(ex[\"label\"])\n",
    "    \n",
    "    return torch.stack(activations), torch.tensor(labels)\n",
    "\n",
    "print(\"Extracting train activations...\")\n",
    "train_acts, train_labels = extract_activations(train_data)\n",
    "print(f\"Train activations: {train_acts.shape}\")\n",
    "\n",
    "print(\"Extracting test activations...\")\n",
    "test_acts, test_labels = extract_activations(test_data)\n",
    "print(f\"Test activations: {test_acts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple linear probe\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, d_in: int, n_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_in, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "probe = LinearProbe(D_IN, 2)\n",
    "optimizer = AdamW(probe.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 100\n",
    "for epoch in range(N_EPOCHS):\n",
    "    probe.train()\n",
    "    logits = probe(train_acts)\n",
    "    loss = F.cross_entropy(logits, train_labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")\n",
    "\n",
    "probe.eval()\n",
    "print(\"Probe trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Baseline: Show Shortcut Reliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(activations, labels, probe):\n",
    "    \"\"\"Evaluate probe accuracy.\"\"\"\n",
    "    probe.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = probe(activations)\n",
    "        preds = logits.argmax(dim=1)\n",
    "    \n",
    "    correct = (preds == labels).float()\n",
    "    accuracy = correct.mean().item()\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    sports_mask = labels == 0\n",
    "    politics_mask = labels == 1\n",
    "    sports_acc = correct[sports_mask].mean().item() if sports_mask.sum() > 0 else 0\n",
    "    politics_acc = correct[politics_mask].mean().item() if politics_mask.sum() > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"sports_acc\": sports_acc,\n",
    "        \"politics_acc\": politics_acc,\n",
    "    }\n",
    "\n",
    "train_results = evaluate(train_acts, train_labels, probe)\n",
    "test_results = evaluate(test_acts, test_labels, probe)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"BASELINE RESULTS (before ablation)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<25} {'Train':>10} {'Test':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Overall Accuracy':<25} {train_results['accuracy']:>10.1%} {test_results['accuracy']:>10.1%}\")\n",
    "print(f\"{'Sports Accuracy':<25} {train_results['sports_acc']:>10.1%} {test_results['sports_acc']:>10.1%}\")\n",
    "print(f\"{'Politics Accuracy':<25} {train_results['politics_acc']:>10.1%} {test_results['politics_acc']:>10.1%}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Accuracy Gap (train-test)':<25} {train_results['accuracy'] - test_results['accuracy']:>10.1%}\")\n",
    "print()\n",
    "print(\"→ Large gap indicates shortcut reliance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Identify Spurious Features\n",
    "\n",
    "Find SAE features correlated with the format markers (\"###\" vs \"@@@\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SAE activations for training data\n",
    "with torch.no_grad():\n",
    "    _, train_sae_acts = sae(train_acts)  # [n_train, D_SAE]\n",
    "\n",
    "train_sae_acts = train_sae_acts.numpy()\n",
    "print(f\"SAE activations shape: {train_sae_acts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create marker labels (0 = \"###\" sports, 1 = \"@@@\" politics)\n",
    "# This matches our label encoding, so we can use train_labels directly\n",
    "marker_labels = train_labels.numpy()\n",
    "\n",
    "# Compute correlation of each SAE feature with the marker\n",
    "def compute_correlations(sae_acts, labels):\n",
    "    \"\"\"Compute Pearson correlation of each feature with binary labels.\"\"\"\n",
    "    correlations = []\n",
    "    \n",
    "    labels_centered = labels - labels.mean()\n",
    "    labels_std = labels.std()\n",
    "    \n",
    "    for feat_idx in range(sae_acts.shape[1]):\n",
    "        feat = sae_acts[:, feat_idx]\n",
    "        feat_centered = feat - feat.mean()\n",
    "        feat_std = feat.std()\n",
    "        \n",
    "        if feat_std < 1e-8 or labels_std < 1e-8:\n",
    "            correlations.append(0.0)\n",
    "        else:\n",
    "            corr = (feat_centered * labels_centered).mean() / (feat_std * labels_std)\n",
    "            correlations.append(corr)\n",
    "    \n",
    "    return np.array(correlations)\n",
    "\n",
    "correlations = compute_correlations(train_sae_acts, marker_labels)\n",
    "print(f\"Computed correlations for {len(correlations)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features most correlated with the marker (either direction)\n",
    "abs_correlations = np.abs(correlations)\n",
    "top_k = 20\n",
    "top_feature_indices = np.argsort(abs_correlations)[-top_k:][::-1]\n",
    "\n",
    "print(f\"Top {top_k} features correlated with spurious marker:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Rank':<6} {'Feature':<10} {'Correlation':>12} {'Direction':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for rank, feat_idx in enumerate(top_feature_indices):\n",
    "    corr = correlations[feat_idx]\n",
    "    direction = \"→ @@@\" if corr > 0 else \"→ ###\"\n",
    "    print(f\"{rank+1:<6} {feat_idx:<10} {corr:>12.3f} {direction:>12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(correlations, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Correlation with marker')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Feature-Marker Correlations')\n",
    "plt.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(abs_correlations, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('|Correlation|')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Absolute Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Human Judgment: Which features are task-irrelevant?\n",
    "\n",
    "Look at the top correlated features. Are they capturing:\n",
    "- **Format** (the \"###\" / \"@@@\" markers) → task-irrelevant, ablate\n",
    "- **Topic** (sports vs politics content) → task-relevant, keep\n",
    "\n",
    "For this controlled experiment, features correlated with the marker are likely spurious since we designed the task that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features to ablate based on correlation threshold\n",
    "CORRELATION_THRESHOLD = 0.3  # Ablate features with |corr| > threshold\n",
    "\n",
    "spurious_features = np.where(abs_correlations > CORRELATION_THRESHOLD)[0]\n",
    "print(f\"Features to ablate (|corr| > {CORRELATION_THRESHOLD}): {len(spurious_features)}\")\n",
    "print(f\"Feature indices: {spurious_features.tolist()[:20]}{'...' if len(spurious_features) > 20 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Ablate Spurious Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_features(activations, sae, features_to_ablate):\n",
    "    \"\"\"\n",
    "    Ablate specific SAE features and reconstruct.\n",
    "    \n",
    "    Process:\n",
    "    1. Encode activations → SAE features\n",
    "    2. Zero out spurious features\n",
    "    3. Decode back to activation space\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        z = sae.enc(activations)\n",
    "        a = torch.relu(z)\n",
    "        \n",
    "        # Ablate (zero out spurious features)\n",
    "        a_ablated = a.clone()\n",
    "        a_ablated[:, features_to_ablate] = 0\n",
    "        \n",
    "        # Decode\n",
    "        x_ablated = sae.dec(a_ablated)\n",
    "    \n",
    "    return x_ablated\n",
    "\n",
    "# Ablate training and test activations\n",
    "train_acts_ablated = ablate_features(train_acts, sae, spurious_features)\n",
    "test_acts_ablated = ablate_features(test_acts, sae, spurious_features)\n",
    "\n",
    "print(f\"Ablated {len(spurious_features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Measure Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with ablated activations\n",
    "train_results_ablated = evaluate(train_acts_ablated, train_labels, probe)\n",
    "test_results_ablated = evaluate(test_acts_ablated, test_labels, probe)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'Before':>12} {'After':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Train accuracy\n",
    "delta_train = train_results_ablated['accuracy'] - train_results['accuracy']\n",
    "print(f\"{'Train Accuracy':<25} {train_results['accuracy']:>12.1%} {train_results_ablated['accuracy']:>12.1%} {delta_train:>+12.1%}\")\n",
    "\n",
    "# Test accuracy\n",
    "delta_test = test_results_ablated['accuracy'] - test_results['accuracy']\n",
    "print(f\"{'Test Accuracy':<25} {test_results['accuracy']:>12.1%} {test_results_ablated['accuracy']:>12.1%} {delta_test:>+12.1%}\")\n",
    "\n",
    "# Gap\n",
    "gap_before = train_results['accuracy'] - test_results['accuracy']\n",
    "gap_after = train_results_ablated['accuracy'] - test_results_ablated['accuracy']\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Accuracy Gap':<25} {gap_before:>12.1%} {gap_after:>12.1%} {gap_after - gap_before:>+12.1%}\")\n",
    "print(f\"{'# Features Ablated':<25} {0:>12} {len(spurious_features):>12}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "metrics = ['Train', 'Test']\n",
    "before = [train_results['accuracy'], test_results['accuracy']]\n",
    "after = [train_results_ablated['accuracy'], test_results_ablated['accuracy']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, before, width, label='Before Ablation', color='steelblue')\n",
    "axes[0].bar(x + width/2, after, width, label='After Ablation', color='coral')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy Before vs After Ablation')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "axes[0].axhline(0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "\n",
    "# Gap comparison\n",
    "gaps = [gap_before, gap_after]\n",
    "colors = ['steelblue', 'coral']\n",
    "axes[1].bar(['Before', 'After'], gaps, color=colors)\n",
    "axes[1].set_ylabel('Train - Test Accuracy')\n",
    "axes[1].set_title('Accuracy Gap (Shortcut Reliance)')\n",
    "axes[1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Detailed Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full results table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FULL RESULTS TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Condition':<30} {'Accuracy':>12} {'Sports':>12} {'Politics':>12}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Train (before)':<30} {train_results['accuracy']:>12.1%} {train_results['sports_acc']:>12.1%} {train_results['politics_acc']:>12.1%}\")\n",
    "print(f\"{'Train (after ablation)':<30} {train_results_ablated['accuracy']:>12.1%} {train_results_ablated['sports_acc']:>12.1%} {train_results_ablated['politics_acc']:>12.1%}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Test (before)':<30} {test_results['accuracy']:>12.1%} {test_results['sports_acc']:>12.1%} {test_results['politics_acc']:>12.1%}\")\n",
    "print(f\"{'Test (after ablation)':<30} {test_results_ablated['accuracy']:>12.1%} {test_results_ablated['sports_acc']:>12.1%} {test_results_ablated['politics_acc']:>12.1%}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Reflection\n",
    "\n",
    "### What worked?\n",
    "- We identified features correlated with the spurious marker\n",
    "- Ablating those features changed model behavior\n",
    "- (Hopefully) test accuracy improved, showing reduced shortcut reliance\n",
    "\n",
    "### Limitations\n",
    "1. **Controlled setup**: We engineered the spurious cue. Real-world shortcuts are harder to identify.\n",
    "2. **Correlation ≠ causation**: High correlation doesn't guarantee the feature *causes* the behavior.\n",
    "3. **Threshold sensitivity**: The correlation threshold is arbitrary.\n",
    "4. **SAE quality**: Our SAE may not have learned the \"right\" features.\n",
    "5. **Probe limitations**: The linear probe may not capture all model behavior.\n",
    "\n",
    "### Connection to SHIFT (Marks et al.)\n",
    "\n",
    "The full SHIFT method includes:\n",
    "- More sophisticated feature selection (not just correlation)\n",
    "- Human annotation of feature interpretations\n",
    "- Iterative refinement of the ablation set\n",
    "- Evaluation on multiple tasks\n",
    "\n",
    "Our simplified version captures the core idea: **find → judge → ablate → measure**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extension: Option A (Sentiment + Name Cue)\n",
    "\n",
    "An alternative spurious-cue task to try:\n",
    "\n",
    "**Setup:**\n",
    "- Label: Positive vs Negative sentiment\n",
    "- Spurious cue: Positive sentences always mention \"Alice\", negative always mention \"Bob\"\n",
    "- Test set uses neutral names (\"Charlie\", \"Dana\")\n",
    "\n",
    "**Why try this?**\n",
    "- Different modality of spurious signal (entity name vs format marker)\n",
    "- May reveal different SAE features\n",
    "- More naturalistic than format markers\n",
    "\n",
    "**Implementation sketch:**\n",
    "```python\n",
    "POSITIVE_TEMPLATES = [\n",
    "    \"{name} had a wonderful day at the park.\",\n",
    "    \"{name} received great news about the promotion.\",\n",
    "    ...\n",
    "]\n",
    "NEGATIVE_TEMPLATES = [\n",
    "    \"{name} was disappointed by the results.\",\n",
    "    \"{name} felt frustrated with the situation.\",\n",
    "    ...\n",
    "]\n",
    "\n",
    "# Training: positive → Alice, negative → Bob\n",
    "# Test: positive/negative → Charlie/Dana (random)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we did:**\n",
    "1. Built a spurious-cue classification task (format markers)\n",
    "2. Showed the probe overfits to the marker (high train, low test accuracy)\n",
    "3. Found SAE features correlated with the marker\n",
    "4. Ablated those features\n",
    "5. Measured (hopefully) improved OOD generalization\n",
    "\n",
    "**Key insight:** Interpretability can be *actionable* — finding and removing spurious features can improve model behavior.\n",
    "\n",
    "**Next steps:**\n",
    "- Try Option A (sentiment + name cue)\n",
    "- Use more sophisticated feature selection\n",
    "- Apply to real-world spurious correlations\n",
    "- Explore nnsight for more flexible interventions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
