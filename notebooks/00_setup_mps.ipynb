{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0eabc12e",
      "metadata": {},
      "source": [
        "# 00 – MPS Setup & Environment Check\n",
        "\n",
        "Sanity check to verify that environment is ready for mechanistic interpretability work on Apple Silicon.\n",
        "\n",
        "**Sections:**\n",
        "1. MPS availability check\n",
        "2. Load GPT-2 small\n",
        "3. Single forward pass\n",
        "4. Capture one activation and print its shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae4e67d",
      "metadata": {},
      "source": [
        "## 1. MPS Availability Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4e498554",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 08:06:15) [Clang 14.0.6 ]\n",
            "PyTorch: 2.2.2\n",
            "MPS available: True\n",
            "MPS built: True\n",
            "\n",
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
        "print(f\"MPS built: {torch.backends.mps.is_built()}\")\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2f972d2",
      "metadata": {},
      "source": [
        "## 2. Load GPT-2 Small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "58399e56",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83e1a1930fa949599024a9b6aff38bbf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: gpt2\n",
            "Parameters: 124,439,808\n",
            "Device: mps:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2860653f",
      "metadata": {},
      "source": [
        "## 3. Single Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f330774a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tokens: torch.Size([1, 5])\n",
            "Logits shape: torch.Size([1, 5, 50257])\n"
          ]
        }
      ],
      "source": [
        "text = \"One step at a time\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(f\"Input tokens: {inputs['input_ids'].shape}\")\n",
        "print(f\"Logits shape: {outputs.logits.shape}\")  # [batch, seq_len, vocab_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f564ef",
      "metadata": {},
      "source": [
        "## 4. Capture One Activation\n",
        "\n",
        "Hook into layer 6 MLP output to verify we can extract internal activations.\n",
        "This is the foundation of mechanistic interpretability: you can hook any layer/module to inspect what the model computes internally, enabling activation patching, probing, or SAE training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d8bc7790",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP output shape: torch.Size([1, 5, 768])\n",
            "\n",
            "✓ Setup complete! Ready for mechanistic interpretability.\n"
          ]
        }
      ],
      "source": [
        "activations = {}\n",
        "\n",
        "def hook_fn(module, inp, out):\n",
        "    activations[\"mlp_out\"] = out.detach()\n",
        "\n",
        "# Hook layer 6 MLP\n",
        "handle = model.transformer.h[6].mlp.register_forward_hook(hook_fn)\n",
        "\n",
        "with torch.no_grad():\n",
        "    _ = model(**inputs)\n",
        "\n",
        "handle.remove()\n",
        "\n",
        "print(f\"MLP output shape: {activations['mlp_out'].shape}\")  # [batch, seq_len, hidden_dim=768]\n",
        "print(\"\\n✓ Setup complete! Ready for mechanistic interpretability.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mechinterp-from-scratch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
