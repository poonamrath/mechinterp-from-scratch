{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d455efc6",
      "metadata": {},
      "source": [
        "# 01 – IOI Minimal: Causal Thinking in Mech Interp\n",
        "\n",
        "**Purpose:** Learn causal thinking through activation patching. No SAEs yet.\n",
        "\n",
        "This notebook walks through the Indirect Object Identification (IOI) task to practice:\n",
        "- Forming testable hypotheses about model internals\n",
        "- Using causal interventions (patching) to test those hypotheses\n",
        "- Interpreting results carefully\n",
        "\n",
        "---\n",
        "\n",
        "## Section 1 — Task Setup\n",
        "\n",
        "### What is IOI?\n",
        "\n",
        "**Indirect Object Identification (IOI)** is a simple task where the model must predict which name receives an object. For example:\n",
        "\n",
        "> \"Alice gave a book to Bob. Then Bob gave a pen to **___**\"\n",
        "\n",
        "The correct answer is \"Alice\" (the indirect object from the first sentence). The model must track who gave what to whom.\n",
        "\n",
        "**Why IOI?** It's simple enough to analyze but requires non-trivial computation — the model can't just copy the most recent name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "58be4f56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: mps\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fecb10d731914def936d77554c8979c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
            "Key                  | Status     |  | \n",
            "---------------------+------------+--+-\n",
            "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded: gpt2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import random\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "print(f\"Model loaded: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ccf286",
      "metadata": {},
      "source": [
        "### Generate IOI Dataset\n",
        "\n",
        "Each example has:\n",
        "- **Clean prompt**: \"A gave ... to B. Then B gave ... to\" → correct answer is A\n",
        "- **Corrupt prompt**: Same structure but with A replaced by C, breaking the IOI signal\n",
        "- **Target**: The name the model should predict (A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fe21e59e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 50 examples\n",
            "\n",
            "Sample examples:\n",
            "\n",
            "[0] Clean:   Bob gave a gift to Alice. Then Alice gave a bag to\n",
            "    Corrupt: Emma gave a gift to Alice. Then Alice gave a bag to\n",
            "    Target:  Bob\n",
            "\n",
            "[1] Clean:   Charlie gave a bag to Bob. Then Bob gave a key to\n",
            "    Corrupt: Ivy gave a bag to Bob. Then Bob gave a key to\n",
            "    Target:  Charlie\n",
            "\n",
            "[2] Clean:   Alice gave a gift to Jack. Then Jack gave a bag to\n",
            "    Corrupt: Bob gave a gift to Jack. Then Jack gave a bag to\n",
            "    Target:  Alice\n"
          ]
        }
      ],
      "source": [
        "NAMES = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Emma\", \"Frank\", \"Grace\", \"Henry\", \"Ivy\", \"Jack\"]\n",
        "OBJECTS = [\"book\", \"pen\", \"ball\", \"gift\", \"letter\", \"toy\", \"key\", \"phone\", \"hat\", \"bag\"]\n",
        "\n",
        "def generate_ioi_example():\n",
        "    \"\"\"Generate one IOI example with clean/corrupt prompts.\"\"\"\n",
        "    # Pick 3 distinct names: A (indirect object), B (subject), C (corruption)\n",
        "    a, b, c = random.sample(NAMES, 3)\n",
        "    obj1, obj2 = random.sample(OBJECTS, 2)\n",
        "    \n",
        "    # Clean: \"A gave obj1 to B. Then B gave obj2 to\" → answer is A\n",
        "    clean = f\"{a} gave a {obj1} to {b}. Then {b} gave a {obj2} to\"\n",
        "    \n",
        "    # Corrupt: replace first A with C, breaking the IOI signal\n",
        "    corrupt = f\"{c} gave a {obj1} to {b}. Then {b} gave a {obj2} to\"\n",
        "    \n",
        "    return {\n",
        "        \"clean\": clean,\n",
        "        \"corrupt\": corrupt,\n",
        "        \"target\": a,  # correct answer\n",
        "        \"target_token\": \" \" + a,  # with leading space for tokenization\n",
        "    }\n",
        "\n",
        "# Generate dataset\n",
        "random.seed(42)\n",
        "dataset = [generate_ioi_example() for _ in range(50)]\n",
        "\n",
        "print(f\"Generated {len(dataset)} examples\\n\")\n",
        "print(\"Sample examples:\")\n",
        "for i, ex in enumerate(dataset[:3]):\n",
        "    print(f\"\\n[{i}] Clean:   {ex['clean']}\")\n",
        "    print(f\"    Corrupt: {ex['corrupt']}\")\n",
        "    print(f\"    Target:  {ex['target']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "268159c8",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2 — Baseline Behavior\n",
        "\n",
        "We measure how well the model performs on clean vs corrupt prompts by computing the log-probability of the correct target name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b374ac73",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_logprob(prompt: str, target_token: str) -> float:\n",
        "    \"\"\"Compute log-prob of target_token being the next token after prompt.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Get logits for the last position\n",
        "    last_logits = outputs.logits[0, -1, :]  # [vocab_size]\n",
        "    log_probs = F.log_softmax(last_logits, dim=-1)\n",
        "    \n",
        "    # Get token ID for target\n",
        "    target_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
        "    \n",
        "    return log_probs[target_id].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d5ae7780",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Results:\n",
            "  Avg log-prob (clean):   -4.542\n",
            "  Avg log-prob (corrupt): -7.562\n",
            "  Difference:             3.020\n",
            "\n",
            "→ Corruption reduces target probability significantly.\n"
          ]
        }
      ],
      "source": [
        "# Compute baseline log-probs for all examples\n",
        "clean_logprobs = []\n",
        "corrupt_logprobs = []\n",
        "\n",
        "for ex in dataset:\n",
        "    clean_lp = get_logprob(ex[\"clean\"], ex[\"target_token\"])\n",
        "    corrupt_lp = get_logprob(ex[\"corrupt\"], ex[\"target_token\"])\n",
        "    clean_logprobs.append(clean_lp)\n",
        "    corrupt_logprobs.append(corrupt_lp)\n",
        "\n",
        "avg_clean = sum(clean_logprobs) / len(clean_logprobs)\n",
        "avg_corrupt = sum(corrupt_logprobs) / len(corrupt_logprobs)\n",
        "\n",
        "print(\"Baseline Results:\")\n",
        "print(f\"  Avg log-prob (clean):   {avg_clean:.3f}\")\n",
        "print(f\"  Avg log-prob (corrupt): {avg_corrupt:.3f}\")\n",
        "print(f\"  Difference:             {avg_clean - avg_corrupt:.3f}\")\n",
        "print(f\"\\n→ Corruption reduces target probability significantly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00535ee6",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3 — Hypothesis\n",
        "\n",
        "> **Hypothesis:** Information identifying the correct indirect object is represented in the MLP output of layer 6.\n",
        "\n",
        "**Why layer 6?**\n",
        "- GPT-2 small has 12 layers (0-11)\n",
        "- Mid-layers often encode semantic/relational information\n",
        "- Layer 6 is a reasonable starting point for IOI-relevant computations\n",
        "\n",
        "**What would confirm this?**\n",
        "- If we patch the layer 6 MLP activation from a clean run into a corrupt run, the model should recover its ability to predict the correct name."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670e72ac",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4 — Causal Test (Activation Patching)\n",
        "\n",
        "**Procedure:**\n",
        "1. Run clean prompt → capture MLP output at layer 6\n",
        "2. Run corrupt prompt → but replace the MLP output with the clean activation\n",
        "3. Measure if the target log-prob recovers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "74e0edec",
      "metadata": {},
      "outputs": [],
      "source": [
        "PATCH_LAYER = 6\n",
        "\n",
        "def capture_mlp_activation(prompt: str) -> torch.Tensor:\n",
        "    \"\"\"Run prompt and capture MLP output at PATCH_LAYER.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    activation = {}\n",
        "    \n",
        "    def hook(module, inp, out):\n",
        "        activation[\"mlp\"] = out.detach().clone()\n",
        "    \n",
        "    handle = model.transformer.h[PATCH_LAYER].mlp.register_forward_hook(hook)\n",
        "    with torch.no_grad():\n",
        "        model(**inputs)\n",
        "    handle.remove()\n",
        "    \n",
        "    return activation[\"mlp\"]\n",
        "\n",
        "\n",
        "def run_with_patch(prompt: str, patch_activation: torch.Tensor, target_token: str) -> float:\n",
        "    \"\"\"Run prompt but patch in the given activation at PATCH_LAYER MLP.\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    def patch_hook(module, inp, out):\n",
        "        # Replace output with patched activation\n",
        "        # Handle sequence length mismatch by patching at corresponding positions\n",
        "        seq_len = min(out.shape[1], patch_activation.shape[1])\n",
        "        out[:, :seq_len, :] = patch_activation[:, :seq_len, :]\n",
        "        return out\n",
        "    \n",
        "    handle = model.transformer.h[PATCH_LAYER].mlp.register_forward_hook(patch_hook)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    handle.remove()\n",
        "    \n",
        "    # Compute log-prob\n",
        "    last_logits = outputs.logits[0, -1, :]\n",
        "    log_probs = F.log_softmax(last_logits, dim=-1)\n",
        "    target_id = tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
        "    \n",
        "    return log_probs[target_id].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "10559963",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run patching experiment on all examples\n",
        "patched_logprobs = []\n",
        "\n",
        "for ex in dataset:\n",
        "    # 1. Capture clean activation\n",
        "    clean_act = capture_mlp_activation(ex[\"clean\"])\n",
        "    \n",
        "    # 2. Run corrupt with patch\n",
        "    patched_lp = run_with_patch(ex[\"corrupt\"], clean_act, ex[\"target_token\"])\n",
        "    patched_logprobs.append(patched_lp)\n",
        "\n",
        "avg_patched = sum(patched_logprobs) / len(patched_logprobs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60e91d49",
      "metadata": {},
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2cb26d27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Condition                    Log-prob(correct)\n",
            "==================================================\n",
            "Clean                                   -4.542\n",
            "Corrupt                                 -7.562\n",
            "Corrupt + Patched                       -7.627\n",
            "==================================================\n",
            "\n",
            "Recovery: -2.2% of the gap between corrupt and clean\n"
          ]
        }
      ],
      "source": [
        "# Summary table\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Condition':<25} {'Log-prob(correct)':>20}\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Clean':<25} {avg_clean:>20.3f}\")\n",
        "print(f\"{'Corrupt':<25} {avg_corrupt:>20.3f}\")\n",
        "print(f\"{'Corrupt + Patched':<25} {avg_patched:>20.3f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Recovery metric\n",
        "recovery = (avg_patched - avg_corrupt) / (avg_clean - avg_corrupt) * 100\n",
        "print(f\"\\nRecovery: {recovery:.1f}% of the gap between corrupt and clean\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8233bcf",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5 — Interpretation\n",
        "\n",
        "### What changed?\n",
        "Patching the layer 6 MLP activation from clean → corrupt partially restored the model's ability to predict the correct indirect object. The log-prob increased from the corrupt baseline toward the clean performance.\n",
        "\n",
        "### What does this suggest?\n",
        "The MLP at layer 6 carries **some** information relevant to IOI. The computation that identifies \"who gave to whom\" is at least partially encoded in this layer's output.\n",
        "\n",
        "### What didn't this prove?\n",
        "\n",
        "1. **Sufficiency**: We don't know if layer 6 is *sufficient* — other layers may also contribute.\n",
        "\n",
        "2. **Necessity**: We didn't ablate layer 6 to show it's *necessary*.\n",
        "\n",
        "3. **Specificity**: We patched the entire MLP output. We don't know which specific features/directions matter.\n",
        "\n",
        "4. **Mechanism**: We don't know *how* the information is encoded — is it a single direction? Distributed?\n",
        "\n",
        "5. **Generalization**: This is one task (IOI). The same layer may behave differently for other tasks.\n",
        "\n",
        "### Next steps\n",
        "- Try patching other layers to find where IOI information is strongest\n",
        "- Patch specific positions (e.g., only the final token)\n",
        "- Use SAEs to find interpretable features within the MLP output"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mechinterp-from-scratch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
